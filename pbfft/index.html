<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="keywords" content="Cubed, jax, GPU, TPU, Python, Xarray, Cloud, Serverless, systems" />
  <meta name="description" content="Towards accelerating FFTs on really
big datasets." />
  <title>The Petabyte FFT: Accelerated Cubed</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    html { -webkit-text-size-adjust: 100%; }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/css/main.css" />
  <meta property="og:image" content="/assets/kraftwerk.webp">
</head>
<body>
<nav>
    <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/faqs">FAQs</a></li>
        <li><a href="/talks">Talks</a></li>
        <li><a href="/blog">Blog</a></li>
        <li><a href="/cv">CV</a></li>
        <li><a href="javascript:if(window.print)window.print()">üñ®Ô∏è</a></li>
    </ul>
</nav>

<header id="title-block-header">
<h1 class="title">The Petabyte FFT: Accelerated Cubed</h1>
<p class="date">2025-01-14T16:00</p>
</header>
<p><em>Originally sketched on September 29, 2023.</em></p>
<p>I'm incredibly excited about <a
href="https://cubed-dev.github.io/cubed/">Cubed</a>. When I first
learned about the project via this <a
href="https://xarray.dev/blog/cubed-xarray">Xarray blog post</a>, I knew
it was worth betting on. To be able to perform array computation
serverlessly -- without having to worry about managing memory (!!) --
seems like the future of data science in the cloud.</p>
<p><em>This slide deck from Tom White, the primary author, does an
excellent job of introducing Cubed.</em></p>
<iframe height="400" width="100%" src="https://cubed-dev.github.io/cubed/cubed-intro.slides.html#/" title="Cubed: An Introduction" loading="lazy"></iframe>

<h2 id="big-arrays--acceleration">Big Arrays &amp; Acceleration</h2>
<p>Maybe the primary source of my excitement was in this project's
potential to change array acceleration. Today, performing computation
with arrays on GPUs/TPUs is still really difficult, even with the
cloud.</p>
<p><img
src="https://preview.redd.it/explain-please-v0-ma2mz5wxftod1.jpeg?auto=webp&amp;s=2b90dfa3b12e064f54333e1080b3dabbad914f48"
alt="All hope is lost" /> ‚Äì <em><a
href="https://www.reddit.com/r/ExplainTheJoke/comments/1fgsbw7/explain_please/">source</a></em></p>
<p>Particularly difficult, until maybe recently, is working with really,
really large arrays on accelerators. For most ML projects, the standard
recommendation is to put as much of the dataset in memory (i.e. RAM) in
order to minimize wasted cycles traversing the memory hierarchy (both
the CPU and (G/T)PU hierarchies). Most ML data pipelines (e.g. tfds) are
designed to efficiently schedule resources (network, disk, RAM, CPUs,
etc.) to keep GPUs saturated. ML examples are often written into
protobuf (tf.records) or flatbuffers (I assume what pytorch uses?) and
then efficiently loaded into memory to keep <a
href="/hazy-research-and-flash-attention">accelerator hardware as busy
as possible</a>.</p>
<p>This becomes much, much trickier to do when you can't dump all your
tf.Examples in memory, or even on disk. This setting is common in the
world of frontier LLMs, but more interesting to me, in scientific
computing settings. How do you train an ML models when your dataset is
over a petabyte in size (or, say, <a
href="https://x.com/shoyer/status/1805735177517416749">6 PiBs</a>)?</p>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:lozmph3nfogiyoi23m4qrxus/app.bsky.feed.post/3leq5wp7z622a" data-bluesky-cid="bafyreid5yz64yt6k3sai2olgihkqutddqcftfwj7hgmj3tavzkpmiw6ati">
    <p lang="en">And if you really want a challenge, this is probably the biggest singular Zarr dataset in existence (~6PiB):

<pre><code>github.com/google-resea...&lt;br&gt;&lt;br&gt;&lt;a href=&quot;https://bsky.app/profile/did:plc:lozmph3nfogiyoi23m4qrxus/post/3leq5wp7z622a?ref_src=embed&quot;&gt;[image or embed]&lt;/a&gt;&lt;/p&gt;&amp;mdash; Al Merose (&lt;a href=&quot;https://bsky.app/profile/did:plc:lozmph3nfogiyoi23m4qrxus?ref_src=embed&quot;&gt;@al.merose.com&lt;/a&gt;) &lt;a href=&quot;https://bsky.app/profile/did:plc:lozmph3nfogiyoi23m4qrxus/post/3leq5wp7z622a?ref_src=embed&quot;&gt;January 1, 2025 at 7:44 PM&lt;/a&gt;</code></pre>
</blockquote>
<script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>

<p>Worse, still: many modern ML tasks don't only make use of a single
dataset, but multiple. Each example is commonly a windowed, jittered
combination of several data sources. In the geosciences, for example,
it's really typical to require combinations of multi-petabyte data
sources. Can you imagine pre-caching these windowed combinations as
protobufs? It literally would <em>factorially</em> expand the amount of
data needed to be stored ‚Äî starting from petabytes!!</p>
<p>This, in a nutshell, is why work improving cloud-optimized data
loaders is so important. Imagine keeping accelerators busy while
creating ML examples just-in-time. Since streaming data into the unit of
compute is inevitable when it can only be stored in cloud buckets, data
loaders keeps accelerators saturated, navigating the memory hierarchy
for you, hopefully with a flexible interface. <a
href="https://earthmover.io/blog/cloud-native-dataloader/"><code>xbatcher</code></a>,
pioneered by <a href="https://earthmover.io/">EarthMover</a> and the <a
href="https://pangeo.io/">Pangeo collective</a>, is one such example of
this infrastructure. Maybe the one I'm most excited to use soon is <a
href="https://github.com/neuralgcm/neuralgcm/issues/97">this internal
data loader</a> developed by the team who created <a
href="https://research.google/blog/fast-accurate-climate-modeling-with-neuralgcm/">NeuralGCM</a>.
(I figure, the more I keep talking about it, the more likely the good
folks at Google will turn it into an open source package üòâ). This is
also why, I argue, investment spent <a
href="https://github.com/earth-mover/icechunk/issues/570">benchmarking
and optimizing the internal components</a> of these data loaders is time
well spent.</p>
<p>Data loading, however, is only one part of the puzzle. Reshaping
data, today, is still quite a big pain, especially for petabyte-scale
inputs. Even with state-of-the-art cloud-optimized data formats,
coalescing source datasets into the appropriate shape for models is
data-engineering intensive. Note, for example, <a
href="https://discourse.pangeo.io/t/shuffling-and-windowing-an-xarray-dataset-for-machine-learning/4790?">Keenan's
experience</a> just the other day (emphasis mine):</p>
<blockquote>
<p>In my work I‚Äôm struggling with providing data from xarray to a
machine learning model. I‚Äôm aware of tools like xbatcher in this¬†<a
href="https://earthmover.io/blog/cloud-native-dataloader/">blog
post</a>¬†and¬†<a
href="https://discourse.pangeo.io/t/efficiently-slicing-random-windows-for-reduced-xarray-dataset/2447">this
other thread</a>. I run into two main sticking points:</p>
<ol type="1">
<li>Randomly shuffling examples is very important. We get vastly
different model performance depending on the order data is provided
during training (see¬†<a
href="https://github.com/s-kganz/ForestLST/blob/main/notebooks/shuffling.ipynb">notebook</a>).</li>
<li><strong>Constructing windowed data with xarray is very memory
intensive.</strong> If I want to slice out all non-NA windows of a
certain size, I have to iterate through small chunks of the data (this
approach was the solution in the thread linked above).</li>
</ol>
<p><strong>My process now is to write an intermediate dataset for a
given window size, drop NAs, shuffle, then train a model. This works,
but then every time I want to modify the input data (e.g. try a 5x5
window instead of 3x3) I have to write a new intermediate
dataset.</strong></p>
</blockquote>
<p>Even with elegant interfaces to express data massaging (Xarray),
managing physical resources (memory, storage) takes up a significant
amount of developer time during ML modeling.</p>
<p>Maybe you're like me, and after reading this, you find yourself
thinking, "Couldn't we automate this data engineering task, especially
given a sufficient Xarray-based specification for what the data should
look like?" If so, then you'll likely share my excitement for Cubed,
which, in my opinion, is a framework perfectly fit to address this
problem!</p>
<p>Cubed, unlike other data engineering systems, is array-aware. Since
it has been designed to respect memory constraints, it can automate
rechunking <a
href="https://www.frontiersin.org/journals/climate/articles/10.3389/fclim.2021.782909/full">ARCO
datasets</a> according to the desired output, no matter how arbitrary.
Internally, Cubed strategically dumps intermediary arrays as Zarr
stores, as has enough understanding of the global operation DAG and
underlying compute resources (namely, RAM), that it solves this game of
memory whack-a-mole for you. That's the dream of Cubed, as far as I
understand it.</p>
<h2
id="networked-array-scheduling-vs-hardware-array-scheduling">Networked
Array Scheduling vs Hardware Array Scheduling</h2>
<p>If you'll permit me to indulge in a moment of possible science
fiction: what fundamentally separates the scheduling happening on the
data engineering side from the internals of the ML training or
inference? From where I stand, I can't help but notice parallels between
the advanced scheduling systems happing in, say, <a
href="https://arxiv.org/abs/2301.13062">XLA</a> or MLIR, from the <a
href="https://github.com/cubed-dev/cubed/issues/333">scheduling
happening within Cubed</a>. If there are parallels, could we find a way
to make them work together?</p>
<p>I think so. And, for these reasons, I am passionate about investing
in finding ways to make Cubed work with accelerators.</p>
<p>The first milestone that I see worthwhile to pursue is to integrate
<a href="https://github.com/cubed-dev/cubed/issues/304">Cubed with
Jax</a>. Jax, for the uninitiated, is as simple as numerical/ML
libraries can get, but no simpler. This is how Jax was introduced to
me:</p>
<blockquote>
<p>Imagine you were tasked with designing the machine learning framework
of the future. What are the fundamental components the framework should
have, given that the goal is to make them run on heterogenous,
accelerated hardware? Well, after careful consideration, it should do
four specific things:</p>
<ol type="1">
<li>It should perform automatic differentiation ("autograd").</li>
<li>It should offer linear algebra primitives (like NumPy)</li>
<li>It should handle randomness (since <a
href="https://pytorch-dev-podcast.simplecast.com/episodes/random-number-generators">randomness
on accelerators is non-trivial</a>)</li>
<li>It should come with standard crypo libraries (for security ‚Äì you
should never roll your own ‚Äì and also because this is non-trivial on
accelerators).</li>
</ol>
<p>That's it. </br> </br> This, more or less, is <a
href="https://jax.readthedocs.io/en/latest/quickstart.html">Jax</a>
.</p>
</blockquote>
<p>‚Äì <em>Paraphrasing a talk I heard once at Google.</em></p>
<p>Jax works, specifically, by providing these four essential components
to Python via a Just-In-Time (JIT) compilation. What this means, more or
less, is you can slap a decorator on your function of NumPy-like code,
and it will turn it into well optimized MLIR/XLA IR (i.e.
intermediate-representation) to run on all sorts of hardware.</p>
<p>(For the record, I think that PyTorch would also work well as the
underlying array acceleration framework. It can do pretty much
everything that I've described here. However, due to my understanding of
the <a href="/ref/long-live-jax">trajectory of PyTorch's development
path</a>, I think Jax is the better bet.)</p>
<p>Once this milestone is achieved, I think it will enable really
interesting stuff. For example, every serverless compute provider
underlying Cubed, in my understanding, offers serverless GPU support.
This infrastructure is getting more popular today for ML model
inference, and thus is becoming a commodity. In practical terms, I
imagine this would mean non-trivially cutting down the time to perform
compute-heavy operations ‚Äî the core array work would all be performed by
GPUs (or TPUs).</p>
<p>Beyond this, I dream about the potential of "cross-scheduler" systems
optimization (i.e. Cubed vs the ML compiler). For example, in the
accelerator literature, a lot of research effort has been invested in
optimizing the memory layouts of data placed into accelerator memory. It
turns out that if you "defrag" the memory layout (i.e. it is
contiguous), it provides an affordance for you to hand-write highly
optimal GPU kernels to process arrays much more efficiently. This, in my
understanding, underlies the <a
href="https://openai.com/index/triton/">Triton compiler</a> from Harvard
and OpenAI.</p>
<p>The task of this hand-tuning is known as "kernel lowering."
Affordances for lowering have been made available in both <a
href="https://github.com/triton-lang/triton">PyTorch via Triton</a> as
well as <a
href="https://jax.readthedocs.io/en/latest/pallas/index.html">Jax via
Pallas</a>. In my opinion, optimizations like these are incredibly worth
it from an accelerator utilization perspective, but seldom invested in
due to <a href="/ml-is-a-huge-search-problem">classic iteration cycles
of ML modeling</a>.</p>
<p>Imagine, if you will, being able to use memory-layout-optimized,
kernel-lowered accelerated array operations, without having to spend
days and days debugging every layer of the stack? This is the kind of
"dual-scheduler" optimization that I hope is possible with Cubed.</p>
<p>At the core of both <a
href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">accelerator
kernels</a> and <a
href="https://zarr.readthedocs.io/en/stable/user-guide/performance.html">managing
Zarr datasets</a>, is the abstraction of the "chunk." (I think the ML
compiler literature might call them "tiles," but they buffer all the
same.) What if Cubed could seamlessly manage memory at every level, from
Zarr chunks to systolic array tiles? <a
href="/thunder-kittens-macro-vs-micro-tiles">Are Zarr chunks not merely
macro tiles?</a></p>
<p><img src="/assets/triton-tiling-hierarchy.jpg"
alt="The Triton Paper&#39;s Tiling Hierarchy" /></p>
<h2 id="towards-a-real-demo">Towards a real demo</h2>
<p>This vision for Cubed may never be tractable. But, to me, it's
something worth fighting for ‚Äì one patch at a time. It's worthwhile to
ground my dreaming in terms of practical milestones. I find these help
me resolve ambiguities, such as what to develop now vs what to put off
for later. In that spirit, I have a demo in mind to build around:</p>
<blockquote>
<p><strong>I want to perform an <a
href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> on a
Petabyte dataset, like <a
href="https://github.com/google-research/arco-era5">ARCO-ERA5</a>. And,
I want to see if I can make it run substantially faster on accelerated
hardware.</strong></p>
</blockquote>
<p>If everything goes right, I think the final code for the whole demo
would look like this:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xarray</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xrft</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cubed <span class="im">import</span> Spec</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> cubed.runtime.executors.lithops <span class="im">import</span> LithopsDagExecutor</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co">(alxmrs): how do you define a GPU-enabled spec?</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>spec <span class="op">=</span> Spec(work_dir<span class="op">=</span><span class="st">&#39;tmp&#39;</span>, allowed_mem<span class="op">=</span><span class="st">&#39;4GB&#39;</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> xarray.open_zarr(</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3&#39;</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    chunks<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    chunked_array_type<span class="op">=</span><span class="st">&#39;cubed&#39;</span>,</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    from_array_kwargs<span class="op">=</span>{<span class="st">&#39;spec&#39;</span>: spec},</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    storage_options<span class="op">=</span><span class="bu">dict</span>(token<span class="op">=</span><span class="st">&#39;anon&#39;</span>),</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>ar_full_37_1h <span class="op">=</span> ds.sel(time<span class="op">=</span><span class="bu">slice</span>(ds.attrs[<span class="st">&#39;valid_time_start&#39;</span>], ds.attrs[<span class="st">&#39;valid_time_stop&#39;</span>]))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>da <span class="op">=</span> ar_full_37_1h[<span class="st">&#39;2m_temperature&#39;</span>]</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>Fda <span class="op">=</span> xrft.dft(da.isel(time<span class="op">=</span><span class="dv">0</span>), dim<span class="op">=</span><span class="st">&#39;lat&#39;</span>, true_phase<span class="op">=</span><span class="va">True</span>, true_amplitude<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>Fda_1 <span class="op">=</span> xrft.idft(Fda, dim<span class="op">=</span><span class="st">&#39;freq_lat&#39;</span>, true_phase<span class="op">=</span><span class="va">True</span>, true_amplitude<span class="op">=</span><span class="va">True</span>, lag<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>Fda_1.compute(executor<span class="op">=</span>LithopsDagExecutor())</span></code></pre></div>
<p><em>Above code is based on examples from <a
href="https://github.com/google-research/arco-era5?tab=readme-ov-file#025-pressure-and-surface-level-data">ARCO-ERA5</a>,
the <a
href="https://xrft.readthedocs.io/en/latest/DFT-iDFT_example.html#A-case-with-real-data">xrft</a>
package, and <a
href="https://xarray.dev/blog/cubed-xarray">Cubed-Xarray</a>. Maybe <a
href="https://github.com/cubed-dev/cubed/issues/438#issuecomment-2598976475">FFTs
will be supported in Cubed</a> one day, who knows!</em></p>
<details>
  <summary>(Umm... are you sure this is a petabyte?)</summary>

<p>No, I am not sure. Ok, I think you're right. Single variables in
ARCO-ERA5 are more like <a
href="https://github.com/alxmrs/xarray-sql/issues/36#issuecomment-2024559470">4
TBs</a>. This is where my lack of atmospheric-physics is showing ‚Äì I'm
not sure how to come up with a code example for computing an FFT in
Xarray that would be at the petabyte scale. However, I suspect it would
be quite possible to make one up. Maybe, it would look similar
processing the model-view data, which is stored in a <a
href="https://github.com/google-research/arco-era5/blob/main/docs/1-Model-Levels-Walkthrough.ipynb">reduced
gaussian grid</a>.</p>
</details>

<p>This challenge is very inspired by <a
href="https://github.com/jax-ml/jax/discussions/13842">this
discussion</a> in the Jax project inquring how to perform FFTs of
&gt;100GB arrays. As of writing, it is currently unsolved. To me,
performing FFTs on large datasets is a sort of "word-count" of
array-processing frameworks: It's a simple enough problem that once
achieved, proves capability of a huge class of problems.</p>
<p>In the farther future, my hope for Cubed is to become a new type of
ML framework, maybe like Anyscale's Ray. With Cubed, I hope that ML
developers never have to worry about <a
href="https://github.com/jax-ml/jax/discussions/10131">OOM errors
again</a> (why not dream big? Though, there are lots of <a
href="https://github.com/cubed-dev/cubed/issues/518">opportunites for
design</a> to make this possible).</p>
<p>Thanks for reading to the end. If this topic interests you, please
reach out to me (or file an <a
href="https://github.com/cubed-dev/cubed/issues">issue in Cubed</a> and
<a href="https://github.com/alxmrs/">tag me</a>).</p>
<footer>
    <ul>
        <li><a href="mailto:al@merose.com">email</a></li>
        <li><a href="https://github.com/alxmrs">github</a></li>
        <li><a href="https://bsky.app/profile/al.merose.com">bsky</a></li>
        <li><a href="https://scholar.google.com/citations?user=9ic0HRsAAAAJ&hl=en">scholar</a></li>
        <li><a href="/rss.xml">rss</a></li>
    </ul>
</footer>
<!-- Verification of Mastodon account -->
<link href="https://hachyderm.io/@al_merose" rel="me">
<!-- Cloudflare Web Analytics -->
<script defer src='https://static.cloudflareinsights.com/beacon.min.js'
        data-cf-beacon='{"token": "6fc27704d9c24c83a33c91f26a5fdcc4"}'>
</script>
<!-- End Cloudflare Web Analytics -->
</body>
</html>
