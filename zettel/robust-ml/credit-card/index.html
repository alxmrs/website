<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Alex Merose</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="/css/main.css" />
</head>
<body>
<nav>
    <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/faqs">FAQs</a></li>
        <li><a href="/talks">Talks</a></li>
        <li><a href="/blog">Blog</a></li>
        <li><a href="/cv">CV</a></li>
        <li><a href="javascript:if(window.print)window.print()">üñ®Ô∏è</a></li>
    </ul>
</nav>

<h1
id="machine-learning-the-high-interest-credit-card-of-technical-debt"><a
href="/https://research.google/pubs/pub43146/">Machine Learning: The
High Interest Credit Card of Technical Debt</a></h1>
<pre><code>@inproceedings{43146,
title   = {Machine Learning: The High Interest Credit Card of Technical Debt},
author  = {D. Sculley and Gary Holt and Daniel Golovin and Eugene Davydov and Todd Phillips and Dietmar Ebner and Vinay Chaudhary and Michael Young},
year    = {2014},
booktitle   = {SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)}
}</code></pre>
<h2 id="summary">Summary</h2>
<p>It's difficult to enforce strict abstraction boundaries for ML
systems, unlike in traditional software engineering, due to its inherent
depends on data (and the data's subsequent quirks).</p>
<h3 id="entanglement">Entanglement</h3>
<p>ML systems are subject to the CACE principle, or "Change Anything
Changes Everything".</p>
<p>One way of mitigating this problem is to "isolate models and serve
ensembles", but this comes with scalability issues. And, within each
model, entanglement issues still abound.</p>
<p>Another mitigation: use tools (or metrics) to visualize effects of
high-dimensional data.</p>
<p>One can consider using sophisticated regularization techniques to
enforce that any changes in performance bear a cost to the objective
function in training. However, this is still not a guarantee, and might
add more debt via system complexity.</p>
<p>Impact of entanglement: "Shipping the first version ... is easy, but
... making subsequent improvements is unexpectedly difficult."</p>
<h3 id="hidden-feedback-loops">Hidden Feedback Loops</h3>
<p>Systems that depend on data from the world, but <em>bear an effect on
the state of the world</em> have an inherent problem of hidden feedback
loops. For example, a model that predicts some user behavior (CTR of
news headlines) at <code>week_t</code>, but measures that same user
behavior at <code>week_{t-1}</code>.</p>
<p>In these situations, the ML system can degrade in a way that's really
hard to notice, esp if models are tested in quick experiments.</p>
<p>Hidden feedback loops should be detected and removed wherever
possible.</p>
<h3 id="undeclared-consumers">Undeclared Consumers</h3>
<p>Without access control for the output of the ML model, some people
can consume the service <em>undeclared</em>. This will tightly couple
other parts of the stack to that model, which can be detrimental to the
system, and can make it hard to change your ML model. Further, it can
introduce a hidden feedback loop!</p>
<h3 id="unstable-data-dependencies">Unstable Data Dependencies</h3>
<p><em>Unstable</em> input signals qualitatively change behavior over
time. Changes and improvements to an input signal may be regularly
rolled out, but this may adversely affect downstream ML models
(CACE).</p>
<p>Mitigation: version signals. Then, updates can be vetted. However,
care is needed to manage versions well, as to not create other technical
debt.</p>
<h3 id="underutilized-data-dependencies">Underutilized Data
Dependencies</h3>
<p><em>Underutilized</em> dependencies are packages that are mostly
unneeded. These are features that provide little incremental value.</p>
<p>These are costly since they make systems unnecessarily vulnerable to
change. These data deps can come is as <em>legacy features</em> (needed
once, but necessary no longer), <em>bundled features</em> (a group of
features is found to be beneficial, but some are not that valuable), or
<em>epsilon-features</em> (added to marginally improve the model
accuracy).</p>
<p>Mitigation: regularly evaluate the effect of removing individual
features.</p>
<h3 id="static-analysis-of-data-dependencies">Static Analysis of Data
Dependencies</h3>
<p>Without static analysis of data dependencies, it can be difficult to
manually track the use of data in the system. It's possible that an
upstream team can change something about the data and have unknown
effects on many consumers. On the other side, a maintainer of some data
source might bot be able to delete / clean up their data out of fear
that they may affect some downstream consumer.</p>
<p>(What if they somehow used Bazel?)</p>
<h3 id="correction-cascades">Correction Cascades</h3>
<p>A model "correction" is where you learn a model <code>a'(a)</code>
that takes another model <code>a</code> and learns a small correction to
solve a different but related problem.</p>
<p>While this is good to create a first version, this creates a system
dependency on model <code>a</code>. It's possible that improving the
accuracy of <code>a</code> could lead to systems-level detriments.</p>
<p>Mitigation: augment <code>a</code> to learn the corrections directly
within the same model by adding features to help that model distinguishs
among the various use cases.</p>
<p><em>(My perspective: it seems like what the authors are recommending
against has since taken off with huge successes ‚Äì Transfer
Learning.)</em></p>
<h3 id="glue-code">Glue Code</h3>
<p><em>Glue code</em> is where a massive amount of supporting code is
written to get data into an out of general-purpose packages. Glue code
can freeze the system to the peculiarities of a specific package.</p>
<p>Sometimes, to reduce glue code, it's better to re-implement the ML
algorithm with considerations for the whole system rather than to re-use
a package from a library. The resulting system may be easier to test
&amp; maintain. You can also inject specific optimizations for your
specific problem (e.g., <a
href="/https://github.com/spotify/annoy">Annoy</a>).</p>
<blockquote>
<p>It may be surprising to the academic community to know that only a
tiny fraction of the code in many machine learning systems is actually
doing ‚Äúmachine learning‚Äù. When we recognize that a mature system might
end up being (at most) 5% machine learning code and (at least) 95% glue
code, reimplementation rather than reuse of a clumsy API looks like a
much better strategy.</p>
</blockquote>
<h3 id="pipeline-jungles">Pipeline Jungles</h3>
<p><em>Pipeline jungles</em> are a special case of glue code concerning
the data ingestion to the ML model. Testing these scrappy pipelines
often require expensive e2e integration tests.</p>
<p>Often, re-implementing the messy pipeline from the ground up is worth
the investment since it can dramatically reduce the costs of maintaining
the old system.</p>
<p>These jungles are often the result of having separate "research" and
"engineering" roles.</p>
<h3 id="dead-experiment-codepaths">Dead Experiment Codepaths</h3>
<p>A common reaction to the above two issues is tweaking the ml
algorithm with conditional branches, one per experiment. Accumulated
control paths create growing technical debt.</p>
<p>Maintaining backwards compatibility is a huge burden. Obsolete code
branches can interact with the latest versions in unexpected ways.</p>
<p>In healthy ML learning systems, experimental code should be well
isolated. This may require rethinking code APIs.</p>
<p>...</p>
<p><a href="/TODO">TODO(Alex): Finish dissecting paper</a></p>
<footer>
    <ul>
        <li><a href="mailto:al@merose.com">email</a></li>
        <li><a href="https://github.com/alxmrs">github</a></li>
        <li><a href="https://bsky.app/profile/al.merose.com">bsky</a></li>
        <li><a href="https://scholar.google.com/citations?user=9ic0HRsAAAAJ&hl=en">scholar</a></li>
        <li><a href="/rss.xml">rss</a></li>
    </ul>
</footer>
<!-- Verification of Mastodon account -->
<link href="https://hachyderm.io/@al_merose" rel="me">
<!-- Cloudflare Web Analytics -->
<script defer src='https://static.cloudflareinsights.com/beacon.min.js'
        data-cf-beacon='{"token": "6fc27704d9c24c83a33c91f26a5fdcc4"}'>
</script>
<!-- End Cloudflare Web Analytics -->
</body>
</html>
