<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2013-07-22" />
  <title>Film in the Era of Google Glass</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="/css/main.css" />
</head>
<body>
<nav>
    <ul>
        <li><a href="/">Home</a></li>
        <li><a href="/faqs">FAQs</a></li>
        <li><a href="/talks">Talks</a></li>
        <li><a href="/blog">Blog</a></li>
        <li><a href="/cv">CV</a></li>
        <li><a href="javascript:if(window.print)window.print()">üñ®Ô∏è</a></li>
    </ul>
</nav>

<header id="title-block-header">
<h1 class="title">Film in the Era of Google Glass</h1>
<p class="date">2013-07-22</p>
</header>
<p>Ubiquitous technologies have already enabled us to capture events
that were never possible before. Not too long ago, one could only see
photographs and movies about the aftermath of meteorites, plane crashes,
explosions, revolutions, and the like. Camera-equipped smartphone have
made it possible to capture catastrophic events in real time, enabling
us to record history as it is happening.</p>
<p>When Google Glass becomes widely used, I expect this phenomena to
proliferate. Constant surveillance would be a casual thing. This faction
of humanity will become living cameras with names and social security
numbers. They will fight to be the first to witness and upload some
object of history, to proudly regard themselves as part of the great
peripheral system in the embodied internet.</p>
<p>Granted, a great deal of this is happening now. Google Glass,
however, seems to be a purer expression of a new trend of consumer
cybernetics. The next step after pushing augmented reality from nuance
to normalcy, in my opinion, will be incorporating biofeedback as a
causal control system.</p>
<p>What peeks my interest the most about this emerging technology is the
potential for revolutionizing art and film. Augmented reality glasses,
such as Google Glass, will usher in a new era of experimental,
independent film.</p>
<p>What if, for instance, a film had no pre-written script, director, or
editor? Instead, just costumes, props, a setting, and highly skilled
long-form improvisors, each equipped with Google Glasses. They could
reenact scenes from history or fantasy in whatever genre of their
choosing. Protagonists would not even be chosen ahead of time: Actors
would be told that there are no lead and non-lead roles; every character
is as important as you make them. Acting troupes would be valued for
their ability to imagine and implement stories as a team.</p>
<p>Director/editors would be granted permission to access the vast
amount of raw footage generated by each actor, no matter how important
or significant they ended up being. The artfulness of the movie would
not only come from the brilliance of the actors, but also from the
director/editors ability to choose the organic narratives generated from
the days of filming. Key to this technique would be the art of
selection.</p>
<p>A single film could produce enough data that teams of
editor/directors could create multiple films, each telling a separate
narrative that relates yet is independent of the others. For instance, a
team of college students could produce an independent, "open-source"
film contest, where they supply the footage of actors performing, lets
say, a murder-mystery. The job of competing editors would be to put the
film together in the most artful way.</p>
<p>This sort of application of Google Glass, to me, would combine data
science and art. There would be so much film that humans might have to
work along algorithms to find the salient aspects of a story across
multiple cameras. The story would have to be pieced together by
software-engineer artists, experts in data compression and design,
neural nets and narrative.</p>
<footer>
    <ul>
        <li><a href="mailto:al@merose.com">email</a></li>
        <li><a href="https://github.com/alxmrs">github</a></li>
        <li><a href="https://bsky.app/profile/al.merose.com">bsky</a></li>
        <li><a href="https://scholar.google.com/citations?user=9ic0HRsAAAAJ&hl=en">scholar</a></li>
        <li><a href="/rss.xml">rss</a></li>
    </ul>
</footer>
<!-- Verification of Mastodon account -->
<link href="https://hachyderm.io/@al_merose" rel="me">
<!-- Cloudflare Web Analytics -->
<script defer src='https://static.cloudflareinsights.com/beacon.min.js'
        data-cf-beacon='{"token": "6fc27704d9c24c83a33c91f26a5fdcc4"}'>
</script>
<!-- End Cloudflare Web Analytics -->
</body>
</html>
